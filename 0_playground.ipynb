{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # We add FileLock here because multiple workers will want to\n",
    "    # download data, and this may cause overwrites since\n",
    "    # DataLoader is not threadsafe.\n",
    "    with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "        trainset = torchvision.datasets.CIFAR10(\n",
    "            root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(config):\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    # To restore a checkpoint, use `session.get_checkpoint()`.\n",
    "    loaded_checkpoint = session.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "           model_state, optimizer_state = torch.load(os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        # Here we save a checkpoint. It is automatically registered with\n",
    "        # Ray Tune and can be accessed through `session.get_checkpoint()`\n",
    "        # API in future iterations.\n",
    "        os.makedirs(\"my_model\", exist_ok=True)\n",
    "        torch.save(\n",
    "            (net.state_dict(), optimizer.state_dict()), \"my_model/checkpoint.pt\")\n",
    "        checkpoint = Checkpoint.from_directory(\"my_model\")\n",
    "        session.report({\"loss\": (val_loss / val_steps), \"accuracy\": correct / total}, checkpoint=checkpoint)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_model(best_result):\n",
    "    best_trained_model = Net(best_result.config[\"l1\"], best_result.config[\"l2\"])\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "\n",
    "    model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = best_trained_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    print(\"Best trial test set accuracy: {}\".format(correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([2, 4, 8, 16]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-30 11:22:17</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:27.21        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.5/27.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 2.000: -1.4740296248435973 | Iter 1.000: -1.5320773645401<br>Logical resource usage: 2.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_cifar_1572f_00000</td><td>RUNNING   </td><td>172.25.212.162:13280</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.0650363</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_cifar_1572f_00001</td><td>TERMINATED</td><td>172.25.212.162:13281</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.0126473</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         58.9053</td><td style=\"text-align: right;\">1.47403</td><td style=\"text-align: right;\">    0.4745</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_cifar pid=13281)\u001b[0m Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/luca/ray_results/train_cifar_2023-07-30_11-20-47/train_cifar_1572f_00001_1_batch_size=16,lr=0.0126_2023-07-30_11-20-50/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/170498071 [00:00<?, ?it/s]\n",
      "  0%|          | 32768/170498071 [00:00<18:42, 151886.89it/s]\n",
      "  0%|          | 65536/170498071 [00:00<18:40, 152133.44it/s]\n",
      "  0%|          | 98304/170498071 [00:00<18:34, 152839.64it/s]\n",
      "  0%|          | 229376/170498071 [00:00<08:31, 332709.71it/s]\n",
      "  0%|          | 458752/170498071 [00:01<04:44, 596811.87it/s]\n",
      "  0%|          | 851968/170498071 [00:01<03:56, 715811.37it/s]\n",
      "  1%|          | 1671168/170498071 [00:01<01:45, 1593809.72it/s]\n",
      "  1%|▏         | 2555904/170498071 [00:01<01:13, 2284854.95it/s]\n",
      "  2%|▏         | 3145728/170498071 [00:02<01:09, 2420286.00it/s]\n",
      "  2%|▏         | 3768320/170498071 [00:02<00:55, 2988489.39it/s]\n",
      "  2%|▏         | 4161536/170498071 [00:02<00:57, 2894371.11it/s]\n",
      "  3%|▎         | 4521984/170498071 [00:02<01:03, 2614205.92it/s]\n",
      "  3%|▎         | 5111808/170498071 [00:02<01:01, 2692175.63it/s]\n",
      "  3%|▎         | 5799936/170498071 [00:03<00:57, 2853634.65it/s]\n",
      "  4%|▍         | 6488064/170498071 [00:03<00:55, 2972146.88it/s]\n",
      "  4%|▍         | 7208960/170498071 [00:03<00:52, 3088270.72it/s]\n",
      "  5%|▍         | 7929856/170498071 [00:03<00:46, 3487284.66it/s]\n",
      "  5%|▍         | 8323072/170498071 [00:03<00:45, 3562301.10it/s]\n",
      "  5%|▌         | 8716288/170498071 [00:03<00:51, 3112444.84it/s]\n",
      "  6%|▌         | 9437184/170498071 [00:04<00:44, 3588059.39it/s]\n",
      "  6%|▌         | 9863168/170498071 [00:04<00:43, 3706757.57it/s]\n",
      "  6%|▌         | 10256384/170498071 [00:04<00:50, 3169409.01it/s]\n",
      "  6%|▋         | 10977280/170498071 [00:04<00:43, 3672468.43it/s]\n",
      "  7%|▋         | 11403264/170498071 [00:04<00:42, 3787858.88it/s]\n",
      "  7%|▋         | 11796480/170498071 [00:04<00:49, 3217827.91it/s]\n",
      "  7%|▋         | 12582912/170498071 [00:04<00:46, 3393612.03it/s]\n",
      "  8%|▊         | 13369344/170498071 [00:05<00:40, 3885443.82it/s]\n",
      "  8%|▊         | 13795328/170498071 [00:05<00:39, 3961036.00it/s]\n",
      "  8%|▊         | 14221312/170498071 [00:05<00:45, 3399860.64it/s]\n",
      "  9%|▉         | 15007744/170498071 [00:05<00:42, 3632772.50it/s]\n",
      "  9%|▉         | 15826944/170498071 [00:05<00:38, 3981069.01it/s]\n",
      " 10%|▉         | 16252928/170498071 [00:05<00:38, 4029716.22it/s]\n",
      " 10%|▉         | 16678912/170498071 [00:06<00:44, 3468638.42it/s]\n",
      " 10%|█         | 17465344/170498071 [00:06<00:35, 4350072.08it/s]\n",
      " 11%|█         | 17956864/170498071 [00:06<00:37, 4020674.00it/s]\n",
      " 11%|█         | 18415616/170498071 [00:06<00:43, 3489441.90it/s]\n",
      " 11%|█         | 19169280/170498071 [00:06<00:41, 3685348.07it/s]\n",
      " 12%|█▏        | 19988480/170498071 [00:06<00:34, 4354342.43it/s]\n",
      " 12%|█▏        | 20480000/170498071 [00:06<00:37, 4037246.89it/s]\n",
      " 12%|█▏        | 20905984/170498071 [00:07<00:42, 3482564.47it/s]\n",
      " 13%|█▎        | 21692416/170498071 [00:07<00:33, 4379982.28it/s]\n",
      " 13%|█▎        | 22183936/170498071 [00:07<00:36, 4036439.27it/s]\n",
      " 13%|█▎        | 22642688/170498071 [00:07<00:42, 3511806.79it/s]\n",
      " 14%|█▎        | 23429120/170498071 [00:07<00:40, 3669808.53it/s]\n",
      " 14%|█▍        | 24248320/170498071 [00:07<00:32, 4440020.89it/s]\n",
      " 15%|█▍        | 24739840/170498071 [00:08<00:35, 4095263.68it/s]\n",
      " 15%|█▍        | 25198592/170498071 [00:08<00:40, 3570222.38it/s]\n",
      " 15%|█▌        | 25985024/170498071 [00:08<00:38, 3790871.14it/s]\n",
      " 16%|█▌        | 26836992/170498071 [00:08<00:34, 4136964.87it/s]\n",
      " 16%|█▌        | 27262976/170498071 [00:08<00:34, 4135423.73it/s]\n",
      " 16%|█▌        | 27688960/170498071 [00:08<00:35, 4052477.50it/s]\n",
      " 16%|█▋        | 28114944/170498071 [00:08<00:35, 4067792.16it/s]\n",
      " 17%|█▋        | 28540928/170498071 [00:08<00:35, 4007805.01it/s]\n",
      " 17%|█▋        | 28966912/170498071 [00:09<00:35, 4030038.12it/s]\n",
      " 17%|█▋        | 29392896/170498071 [00:09<00:35, 3971610.24it/s]\n",
      " 17%|█▋        | 29818880/170498071 [00:09<00:35, 4003825.92it/s]\n",
      " 18%|█▊        | 30244864/170498071 [00:09<00:35, 3964060.07it/s]\n",
      " 18%|█▊        | 30670848/170498071 [00:09<00:35, 3993943.80it/s]\n",
      " 18%|█▊        | 31129600/170498071 [00:09<00:38, 3601010.27it/s]\n",
      " 19%|█▉        | 31981568/170498071 [00:09<00:36, 3764809.99it/s]\n",
      " 19%|█▉        | 32833536/170498071 [00:10<00:35, 3836239.70it/s]\n",
      " 20%|█▉        | 33685504/170498071 [00:10<00:32, 4151173.61it/s]\n",
      " 20%|██        | 34111488/170498071 [00:10<00:32, 4141256.19it/s]\n",
      " 20%|██        | 34537472/170498071 [00:10<00:33, 4064753.01it/s]\n",
      " 21%|██        | 34963456/170498071 [00:10<00:33, 4071443.15it/s]\n",
      " 21%|██        | 35422208/170498071 [00:10<00:36, 3700387.65it/s]\n",
      " 21%|██▏       | 36241408/170498071 [00:10<00:29, 4478052.70it/s]\n",
      " 22%|██▏       | 36700160/170498071 [00:11<00:33, 4006117.94it/s]\n",
      " 22%|██▏       | 37126144/170498071 [00:11<00:36, 3630463.24it/s]\n",
      " 22%|██▏       | 37978112/170498071 [00:11<00:35, 3761506.21it/s]\n",
      " 23%|██▎       | 38830080/170498071 [00:11<00:31, 4131094.12it/s]\n",
      " 23%|██▎       | 39256064/170498071 [00:11<00:32, 4098028.69it/s]\n",
      " 23%|██▎       | 39714816/170498071 [00:11<00:34, 3760816.87it/s]\n",
      " 24%|██▍       | 40566784/170498071 [00:12<00:31, 4150341.10it/s]\n",
      " 24%|██▍       | 40992768/170498071 [00:12<00:31, 4117663.72it/s]\n",
      " 24%|██▍       | 41418752/170498071 [00:12<00:31, 4081650.87it/s]\n",
      " 25%|██▍       | 41844736/170498071 [00:12<00:31, 4059726.79it/s]\n",
      " 25%|██▍       | 42303488/170498071 [00:12<00:31, 4090778.73it/s]\n",
      " 25%|██▌       | 42729472/170498071 [00:12<00:31, 4057262.72it/s]\n",
      " 25%|██▌       | 43188224/170498071 [00:12<00:34, 3684465.10it/s]\n",
      " 26%|██▌       | 44040192/170498071 [00:12<00:30, 4154564.66it/s]\n",
      " 26%|██▌       | 44466176/170498071 [00:12<00:30, 4122905.87it/s]\n",
      " 26%|██▋       | 44924928/170498071 [00:13<00:30, 4138106.97it/s]\n",
      " 27%|██▋       | 45350912/170498071 [00:13<00:30, 4108763.92it/s]\n",
      " 27%|██▋       | 45809664/170498071 [00:13<00:30, 4124335.51it/s]\n",
      " 27%|██▋       | 46235648/170498071 [00:13<00:30, 4100456.17it/s]\n",
      " 27%|██▋       | 46694400/170498071 [00:13<00:29, 4131291.98it/s]\n",
      " 28%|██▊       | 47120384/170498071 [00:13<00:30, 4099968.92it/s]\n",
      " 28%|██▊       | 47579136/170498071 [00:13<00:29, 4130550.62it/s]\n",
      " 28%|██▊       | 48005120/170498071 [00:13<00:29, 4111978.91it/s]\n",
      " 28%|██▊       | 48496640/170498071 [00:13<00:29, 4195282.95it/s]\n",
      " 29%|██▊       | 48922624/170498071 [00:14<00:29, 4161612.04it/s]\n",
      " 29%|██▉       | 49414144/170498071 [00:14<00:28, 4239035.26it/s]\n",
      " 29%|██▉       | 49840128/170498071 [00:14<00:28, 4189568.48it/s]\n",
      " 30%|██▉       | 50331648/170498071 [00:14<00:28, 4261433.98it/s]\n",
      " 30%|██▉       | 50757632/170498071 [00:14<00:28, 4219016.06it/s]\n",
      " 30%|███       | 51249152/170498071 [00:14<00:27, 4283165.73it/s]\n",
      " 30%|███       | 51707904/170498071 [00:14<00:27, 4288483.53it/s]\n",
      " 31%|███       | 52199424/170498071 [00:14<00:27, 4349039.00it/s]\n",
      " 31%|███       | 52658176/170498071 [00:14<00:27, 4329410.98it/s]\n",
      " 31%|███       | 53149696/170498071 [00:15<00:26, 4385204.89it/s]\n",
      " 31%|███▏      | 53608448/170498071 [00:15<00:26, 4357244.74it/s]\n",
      " 32%|███▏      | 54099968/170498071 [00:15<00:26, 4419078.52it/s]\n",
      " 32%|███▏      | 54558720/170498071 [00:15<00:26, 4386546.55it/s]\n",
      " 32%|███▏      | 55050240/170498071 [00:15<00:25, 4467944.92it/s]\n",
      " 33%|███▎      | 55508992/170498071 [00:15<00:26, 4397260.49it/s]\n",
      " 33%|███▎      | 56033280/170498071 [00:15<00:25, 4535087.64it/s]\n",
      " 33%|███▎      | 56492032/170498071 [00:15<00:25, 4456778.13it/s]\n",
      " 33%|███▎      | 57049088/170498071 [00:15<00:24, 4634343.30it/s]\n",
      " 34%|███▎      | 57540608/170498071 [00:15<00:24, 4601863.10it/s]\n",
      " 34%|███▍      | 58064896/170498071 [00:16<00:23, 4689537.64it/s]\n",
      " 34%|███▍      | 58556416/170498071 [00:16<00:24, 4643222.72it/s]\n",
      " 35%|███▍      | 59080704/170498071 [00:16<00:23, 4753171.84it/s]\n",
      " 35%|███▍      | 59572224/170498071 [00:16<00:23, 4674725.35it/s]\n",
      " 35%|███▌      | 60129280/170498071 [00:16<00:22, 4835055.63it/s]\n",
      " 36%|███▌      | 60620800/170498071 [00:16<00:23, 4742386.68it/s]\n",
      " 36%|███▌      | 61210624/170498071 [00:16<00:22, 4945497.28it/s]\n",
      " 36%|███▌      | 61734912/170498071 [00:16<00:22, 4886408.14it/s]\n",
      " 37%|███▋      | 62291968/170498071 [00:16<00:21, 5021430.43it/s]\n",
      " 37%|███▋      | 62816256/170498071 [00:17<00:21, 4924360.36it/s]\n",
      " 37%|███▋      | 63406080/170498071 [00:17<00:20, 5125921.91it/s]\n",
      " 37%|███▋      | 63930368/170498071 [00:17<00:21, 5004413.10it/s]\n",
      " 38%|███▊      | 64520192/170498071 [00:17<00:20, 5212294.10it/s]\n",
      " 38%|███▊      | 65044480/170498071 [00:17<00:20, 5069892.66it/s]\n",
      " 39%|███▊      | 65667072/170498071 [00:17<00:19, 5356333.52it/s]\n",
      " 39%|███▉      | 66224128/170498071 [00:17<00:20, 5192853.68it/s]\n",
      " 39%|███▉      | 66846720/170498071 [00:17<00:18, 5464375.77it/s]\n",
      " 40%|███▉      | 67403776/170498071 [00:17<00:19, 5286451.40it/s]\n",
      " 40%|███▉      | 68059136/170498071 [00:18<00:18, 5576124.08it/s]\n",
      " 40%|████      | 68648960/170498071 [00:18<00:18, 5441894.26it/s]\n",
      " 41%|████      | 69304320/170498071 [00:18<00:19, 5156954.27it/s]\n",
      " 41%|████      | 70221824/170498071 [00:18<00:16, 6176895.66it/s]\n",
      " 42%|████▏     | 70877184/170498071 [00:18<00:18, 5346046.97it/s]\n",
      " 42%|████▏     | 71827456/170498071 [00:18<00:16, 6044613.07it/s]\n",
      " 43%|████▎     | 72482816/170498071 [00:18<00:16, 5878917.22it/s]\n",
      " 43%|████▎     | 73203712/170498071 [00:18<00:17, 5642219.17it/s]\n",
      " 43%|████▎     | 74121216/170498071 [00:19<00:14, 6478337.72it/s]\n",
      " 44%|████▍     | 74809344/170498071 [00:19<00:16, 5714560.75it/s]\n",
      " 45%|████▍     | 75890688/170498071 [00:19<00:14, 6547717.49it/s]\n",
      " 45%|████▍     | 76578816/170498071 [00:19<00:14, 6291545.07it/s]\n",
      " 45%|████▌     | 77365248/170498071 [00:19<00:15, 6113875.11it/s]\n",
      " 46%|████▌     | 78282752/170498071 [00:19<00:13, 6841170.39it/s]\n",
      " 46%|████▋     | 79003648/170498071 [00:19<00:14, 6115495.70it/s]\n",
      " 47%|████▋     | 80183296/170498071 [00:19<00:12, 7505802.62it/s]\n",
      " 48%|████▊     | 81002496/170498071 [00:20<00:13, 6695015.63it/s]\n",
      " 48%|████▊     | 81887232/170498071 [00:20<00:13, 6610875.74it/s]\n",
      " 49%|████▊     | 82837504/170498071 [00:20<00:12, 7276425.88it/s]\n",
      " 49%|████▉     | 83623936/170498071 [00:20<00:13, 6630774.47it/s]\n",
      " 50%|████▉     | 84803584/170498071 [00:20<00:10, 7893325.21it/s]\n",
      " 50%|█████     | 85655552/170498071 [00:20<00:11, 7162775.14it/s]\n",
      " 51%|█████     | 86704128/170498071 [00:20<00:10, 7958914.09it/s]\n",
      " 51%|█████▏    | 87556096/170498071 [00:20<00:11, 7465550.54it/s]\n",
      " 52%|█████▏    | 88506368/170498071 [00:21<00:11, 7444013.99it/s]\n",
      " 52%|█████▏    | 89456640/170498071 [00:21<00:10, 7936368.72it/s]\n",
      " 53%|█████▎    | 90308608/170498071 [00:21<00:10, 7724955.92it/s]\n",
      " 54%|█████▎    | 91291648/170498071 [00:21<00:09, 8242699.53it/s]\n",
      " 54%|█████▍    | 92143616/170498071 [00:21<00:09, 8025751.06it/s]\n",
      " 55%|█████▍    | 93126656/170498071 [00:21<00:09, 8484248.19it/s]\n",
      " 55%|█████▌    | 94011392/170498071 [00:21<00:09, 8347970.33it/s]\n",
      " 56%|█████▌    | 94961664/170498071 [00:21<00:08, 8641169.76it/s]\n",
      " 56%|█████▋    | 95944704/170498071 [00:21<00:08, 8633686.83it/s]\n",
      " 57%|█████▋    | 96894976/170498071 [00:22<00:08, 8872758.97it/s]\n",
      " 57%|█████▋    | 97943552/170498071 [00:22<00:08, 8982653.04it/s]\n",
      " 58%|█████▊    | 98893824/170498071 [00:22<00:07, 9092907.84it/s]\n",
      " 59%|█████▉    | 100958208/170498071 [00:22<00:07, 9341538.84it/s]\n",
      " 60%|█████▉    | 102137856/170498071 [00:22<00:07, 9612330.96it/s]\n",
      " 60%|██████    | 103120896/170498071 [00:22<00:07, 9591181.04it/s]\n",
      " 61%|██████    | 104300544/170498071 [00:22<00:06, 10207427.93it/s]\n",
      " 62%|██████▏   | 105349120/170498071 [00:22<00:06, 9762185.58it/s] \n",
      " 62%|██████▏   | 106528768/170498071 [00:23<00:06, 10306571.06it/s]\n",
      " 63%|██████▎   | 107577344/170498071 [00:23<00:06, 10016437.54it/s]\n",
      " 64%|██████▍   | 108756992/170498071 [00:23<00:05, 10507923.34it/s]\n",
      " 64%|██████▍   | 109838336/170498071 [00:23<00:05, 10335733.56it/s]\n",
      " 65%|██████▌   | 111017984/170498071 [00:23<00:05, 10723460.64it/s]\n",
      " 66%|██████▌   | 112099328/170498071 [00:23<00:05, 10667259.14it/s]\n",
      " 66%|██████▋   | 113278976/170498071 [00:23<00:05, 10983794.28it/s]\n",
      " 67%|██████▋   | 114425856/170498071 [00:23<00:05, 11043395.84it/s]\n",
      " 68%|██████▊   | 115605504/170498071 [00:23<00:04, 11254910.44it/s]\n",
      " 68%|██████▊   | 116785152/170498071 [00:23<00:04, 11371881.31it/s]\n",
      " 69%|██████▉   | 117964800/170498071 [00:24<00:04, 11447620.95it/s]\n",
      " 70%|██████▉   | 119144448/170498071 [00:24<00:04, 11516857.53it/s]\n",
      " 71%|███████   | 120324096/170498071 [00:24<00:04, 11555663.26it/s]\n",
      " 71%|███████▏  | 121503744/170498071 [00:24<00:04, 11607930.49it/s]\n",
      " 72%|███████▏  | 122683392/170498071 [00:24<00:06, 7556137.59it/s] \n",
      " 73%|███████▎  | 123633664/170498071 [00:24<00:06, 7807350.34it/s]\n",
      " 73%|███████▎  | 124649472/170498071 [00:24<00:06, 7515659.68it/s]\n",
      " 74%|███████▎  | 125501440/170498071 [00:25<00:08, 5333872.21it/s]\n",
      " 74%|███████▍  | 126189568/170498071 [00:25<00:08, 5240528.23it/s]\n",
      " 75%|███████▍  | 127827968/170498071 [00:25<00:05, 7163415.48it/s]\n",
      " 75%|███████▌  | 128679936/170498071 [00:25<00:06, 6735720.10it/s]\n",
      " 76%|███████▌  | 129466368/170498071 [00:25<00:06, 6206444.93it/s]\n",
      " 77%|███████▋  | 130580480/170498071 [00:25<00:05, 7265196.86it/s]\n",
      " 77%|███████▋  | 131399680/170498071 [00:26<00:06, 6506398.83it/s]\n",
      " 78%|███████▊  | 132349952/170498071 [00:26<00:06, 6205799.21it/s]\n",
      " 78%|███████▊  | 133496832/170498071 [00:26<00:05, 7317853.64it/s]\n",
      " 79%|███████▉  | 134316032/170498071 [00:26<00:05, 6584537.01it/s]\n",
      " 79%|███████▉  | 135266304/170498071 [00:26<00:05, 6284367.93it/s]\n",
      " 80%|████████  | 136413184/170498071 [00:26<00:04, 7409099.78it/s]\n",
      " 80%|████████  | 137232384/170498071 [00:26<00:04, 6663146.87it/s]\n",
      " 81%|████████  | 138248192/170498071 [00:26<00:04, 7437316.05it/s]\n",
      " 82%|████████▏ | 139067392/170498071 [00:27<00:04, 6925059.40it/s]\n",
      " 82%|████████▏ | 139821056/170498071 [00:27<00:04, 6348442.58it/s]\n",
      " 83%|████████▎ | 140967936/170498071 [00:27<00:03, 7554516.44it/s]\n",
      " 83%|████████▎ | 141787136/170498071 [00:27<00:04, 6817439.13it/s]\n",
      " 84%|████████▍ | 142802944/170498071 [00:27<00:03, 7600742.61it/s]\n",
      " 84%|████████▍ | 143622144/170498071 [00:27<00:03, 7014911.27it/s]\n",
      " 85%|████████▍ | 144441344/170498071 [00:27<00:03, 6551825.81it/s]\n",
      " 85%|████████▌ | 145555456/170498071 [00:28<00:03, 7633644.45it/s]\n",
      " 86%|████████▌ | 146374656/170498071 [00:28<00:03, 6943220.58it/s]\n",
      " 86%|████████▋ | 147423232/170498071 [00:28<00:02, 7787443.07it/s]\n",
      " 87%|████████▋ | 148275200/170498071 [00:28<00:03, 7193797.93it/s]\n",
      " 88%|████████▊ | 149225472/170498071 [00:28<00:03, 6774735.48it/s]\n",
      " 88%|████████▊ | 150372352/170498071 [00:28<00:02, 7875054.99it/s]\n",
      " 89%|████████▊ | 151224320/170498071 [00:28<00:02, 7145486.67it/s]\n",
      " 89%|████████▉ | 152305664/170498071 [00:28<00:02, 7987761.09it/s]\n",
      " 90%|████████▉ | 153157632/170498071 [00:29<00:02, 7357444.46it/s]\n",
      " 90%|█████████ | 154042368/170498071 [00:29<00:02, 7318261.34it/s]\n",
      " 91%|█████████ | 154861568/170498071 [00:29<00:02, 7534212.37it/s]\n",
      " 91%|█████████▏| 155680768/170498071 [00:29<00:02, 6933901.54it/s]\n",
      " 92%|█████████▏| 156827648/170498071 [00:29<00:01, 7983210.38it/s]\n",
      " 92%|█████████▏| 157679616/170498071 [00:29<00:01, 7266409.83it/s]\n",
      " 93%|█████████▎| 158760960/170498071 [00:29<00:01, 8149248.66it/s]\n",
      " 94%|█████████▎| 159645696/170498071 [00:29<00:01, 7468104.88it/s]\n",
      " 94%|█████████▍| 160595968/170498071 [00:30<00:01, 7479362.71it/s]\n",
      " 95%|█████████▍| 161447936/170498071 [00:30<00:01, 7691181.37it/s]\n",
      " 95%|█████████▌| 162267136/170498071 [00:30<00:01, 7100901.45it/s]\n",
      " 96%|█████████▌| 163381248/170498071 [00:30<00:00, 8115474.05it/s]\n",
      " 96%|█████████▋| 164233216/170498071 [00:30<00:00, 7311000.35it/s]\n",
      " 97%|█████████▋| 165347328/170498071 [00:30<00:00, 8261167.91it/s]\n",
      " 97%|█████████▋| 166232064/170498071 [00:30<00:00, 7532961.48it/s]\n",
      " 98%|█████████▊| 167215104/170498071 [00:30<00:00, 7592674.79it/s]\n",
      " 99%|█████████▊| 168034304/170498071 [00:31<00:00, 7733505.86it/s]\n",
      " 99%|█████████▉| 168886272/170498071 [00:31<00:00, 7622880.63it/s]\n",
      "100%|█████████▉| 169705472/170498071 [00:31<00:00, 7763143.00it/s]\n",
      "100%|██████████| 170498071/170498071 [00:31<00:00, 5447515.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_cifar pid=13281)\u001b[0m Extracting /home/luca/ray_results/train_cifar_2023-07-30_11-20-47/train_cifar_1572f_00001_1_batch_size=16,lr=0.0126_2023-07-30_11-20-50/data/cifar-10-python.tar.gz to /home/luca/ray_results/train_cifar_2023-07-30_11-20-47/train_cifar_1572f_00001_1_batch_size=16,lr=0.0126_2023-07-30_11-20-50/data\n",
      "\u001b[2m\u001b[36m(train_cifar pid=13281)\u001b[0m Files already downloaded and verified\n",
      "\u001b[2m\u001b[36m(train_cifar pid=13280)\u001b[0m Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/luca/ray_results/train_cifar_2023-07-30_11-20-47/train_cifar_1572f_00000_0_batch_size=16,lr=0.0650_2023-07-30_11-20-50/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/170498071 [00:00<?, ?it/s]\n",
      "  0%|          | 32768/170498071 [00:00<37:15, 76252.20it/s]\n",
      "  0%|          | 65536/170498071 [00:00<26:20, 107805.59it/s]\n",
      "  0%|          | 98304/170498071 [00:00<22:54, 123998.15it/s]\n",
      "  0%|          | 131072/170498071 [00:01<28:34, 99346.47it/s]\n",
      "  0%|          | 163840/170498071 [00:01<31:42, 89538.18it/s]\n",
      "  0%|          | 196608/170498071 [00:02<33:37, 84396.99it/s]\n",
      "  0%|          | 229376/170498071 [00:02<34:48, 81539.17it/s]\n",
      "  0%|          | 262144/170498071 [00:02<30:53, 91854.34it/s]\n",
      "  0%|          | 294912/170498071 [00:03<32:52, 86277.56it/s]\n",
      "  0%|          | 327680/170498071 [00:03<33:06, 85665.66it/s]\n",
      "  0%|          | 360448/170498071 [00:04<35:33, 79731.71it/s]\n",
      "  0%|          | 393216/170498071 [00:04<36:05, 78567.17it/s]\n",
      "  0%|          | 425984/170498071 [00:04<32:29, 87250.93it/s]\n",
      "  0%|          | 458752/170498071 [00:05<32:16, 87825.95it/s]\n",
      "  0%|          | 491520/170498071 [00:05<36:28, 77669.16it/s]\n",
      "  0%|          | 524288/170498071 [00:05<30:04, 94213.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_cifar pid=13281)\u001b[0m [1,  2000] loss: 1.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 557056/170498071 [00:06<33:15, 85173.49it/s]\n",
      "  0%|          | 589824/170498071 [00:06<32:54, 86070.58it/s]\n",
      "  0%|          | 622592/170498071 [00:07<32:24, 87381.30it/s]\n",
      "  0%|          | 655360/170498071 [00:07<30:00, 94353.83it/s]\n",
      "  0%|          | 688128/170498071 [00:07<33:47, 83762.35it/s]\n",
      "  0%|          | 720896/170498071 [00:08<34:49, 81262.86it/s]\n",
      "  0%|          | 753664/170498071 [00:08<35:33, 79557.36it/s]\n",
      "  0%|          | 786432/170498071 [00:09<41:36, 67972.42it/s]\n",
      "  0%|          | 819200/170498071 [00:09<40:18, 70150.82it/s]\n",
      "  0%|          | 851968/170498071 [00:10<39:20, 71868.19it/s]\n",
      "  1%|          | 884736/170498071 [00:10<38:42, 73023.70it/s]\n",
      "  1%|          | 917504/170498071 [00:11<39:29, 71563.01it/s]\n",
      "  1%|          | 950272/170498071 [00:11<38:46, 72867.05it/s]\n",
      "  1%|          | 983040/170498071 [00:12<45:01, 62748.44it/s]\n",
      "  1%|          | 1015808/170498071 [00:12<42:38, 66232.72it/s]\n",
      "  1%|          | 1048576/170498071 [00:13<52:08, 54163.07it/s]\n",
      "  1%|          | 1081344/170498071 [00:14<51:59, 54300.79it/s]\n",
      "  1%|          | 1114112/170498071 [00:14<52:48, 53463.85it/s]\n",
      "  1%|          | 1146880/170498071 [00:15<53:59, 52278.19it/s]\n",
      "  1%|          | 1179648/170498071 [00:16<1:04:03, 44048.29it/s]\n",
      "  1%|          | 1212416/170498071 [00:17<58:46, 48010.04it/s]  \n",
      "  1%|          | 1245184/170498071 [00:17<57:49, 48785.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_cifar pid=13281)\u001b[0m [2,  2000] loss: 1.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1277952/170498071 [00:18<54:23, 51856.53it/s]\n",
      "  1%|          | 1310720/170498071 [00:18<54:44, 51506.93it/s]\n",
      "  1%|          | 1343488/170498071 [00:19<53:56, 52270.54it/s]\n",
      "  1%|          | 1376256/170498071 [00:20<1:01:06, 46126.86it/s]\n",
      "  1%|          | 1409024/170498071 [00:21<1:03:52, 44115.53it/s]\n",
      "  1%|          | 1441792/170498071 [00:22<1:05:10, 43236.43it/s]\n",
      "  1%|          | 1474560/170498071 [00:22<1:04:04, 43962.87it/s]\n",
      "  1%|          | 1507328/170498071 [00:23<1:12:15, 38979.74it/s]\n",
      "  1%|          | 1540096/170498071 [00:24<1:19:45, 35304.54it/s]\n",
      "  1%|          | 1572864/170498071 [00:25<1:13:55, 38085.91it/s]\n",
      "  1%|          | 1605632/170498071 [00:26<1:08:23, 41159.18it/s]\n",
      "  1%|          | 1638400/170498071 [00:26<1:02:32, 44994.69it/s]\n",
      "  1%|          | 1671168/170498071 [00:27<55:24, 50775.38it/s]  \n",
      "  1%|          | 1703936/170498071 [00:27<54:36, 51518.14it/s]\n",
      "  1%|          | 1736704/170498071 [00:28<52:40, 53401.31it/s]\n",
      "  1%|          | 1769472/170498071 [00:28<47:57, 58628.02it/s]\n",
      "  1%|          | 1802240/170498071 [00:29<44:39, 62951.99it/s]\n",
      "  1%|          | 1835008/170498071 [00:29<43:34, 64518.63it/s]\n",
      "  1%|          | 1867776/170498071 [00:30<45:57, 61147.53it/s]\n",
      "  1%|          | 1900544/170498071 [00:31<49:58, 56227.50it/s]\n",
      "  1%|          | 1933312/170498071 [00:31<53:17, 52719.16it/s]\n",
      "  1%|          | 1966080/170498071 [00:32<1:04:33, 43513.75it/s]\n",
      "  1%|          | 1998848/170498071 [00:33<1:03:40, 44106.05it/s]\n",
      "  1%|          | 2031616/170498071 [00:34<1:14:12, 37835.70it/s]\n",
      "  1%|          | 2064384/170498071 [00:35<1:14:06, 37880.89it/s]\n",
      "  1%|          | 2097152/170498071 [00:36<1:08:28, 40993.34it/s]\n",
      "  1%|          | 2129920/170498071 [00:37<1:09:15, 40520.36it/s]\n",
      "  1%|▏         | 2162688/170498071 [00:37<1:05:03, 43119.51it/s]\n",
      "  1%|▏         | 2195456/170498071 [00:38<1:10:24, 39839.78it/s]\n",
      "  1%|▏         | 2228224/170498071 [00:40<1:40:46, 27830.70it/s]\n",
      "  1%|▏         | 2260992/170498071 [00:42<1:49:13, 25669.39it/s]\n",
      "  1%|▏         | 2293760/170498071 [00:43<1:41:23, 27648.51it/s]\n",
      "  1%|▏         | 2326528/170498071 [00:44<1:34:49, 29556.92it/s]\n",
      "  1%|▏         | 2359296/170498071 [00:44<1:23:56, 33384.71it/s]\n",
      "  1%|▏         | 2392064/170498071 [00:45<1:23:52, 33405.83it/s]\n",
      "  1%|▏         | 2424832/170498071 [00:46<1:11:15, 39307.12it/s]\n",
      "  1%|▏         | 2457600/170498071 [00:46<1:05:27, 42784.67it/s]\n",
      "2023-07-30 11:22:17,754\tWARNING tune.py:192 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "  1%|▏         | 2490368/170498071 [00:47<58:52, 47560.55it/s]  \n",
      "  1%|▏         | 2523136/170498071 [00:48<1:02:19, 44923.79it/s]\n",
      "  1%|▏         | 2555904/170498071 [00:48<54:39, 51209.86it/s]  \n",
      "  2%|▏         | 2588672/170498071 [00:49<51:28, 54359.61it/s]\n",
      "  2%|▏         | 2621440/170498071 [00:49<48:18, 57925.30it/s]\n",
      "  2%|▏         | 2654208/170498071 [00:50<46:55, 59610.20it/s]\n",
      "  2%|▏         | 2686976/170498071 [00:50<43:52, 63735.27it/s]\n",
      "  2%|▏         | 2719744/170498071 [00:51<45:09, 61913.52it/s]\n",
      "  2%|▏         | 2752512/170498071 [00:51<44:45, 62473.02it/s]\n",
      "  2%|▏         | 2785280/170498071 [00:52<42:20, 66017.27it/s]\n",
      "  2%|▏         | 2818048/170498071 [00:52<39:37, 70530.50it/s]\n",
      "  2%|▏         | 2850816/170498071 [00:52<38:44, 72110.54it/s]\n",
      "  2%|▏         | 2883584/170498071 [00:53<38:08, 73256.94it/s]\n",
      "  2%|▏         | 2916352/170498071 [00:53<37:45, 73986.16it/s]\n",
      "  2%|▏         | 2949120/170498071 [00:54<39:26, 70803.08it/s]\n",
      "  2%|▏         | 2981888/170498071 [00:54<37:36, 74242.18it/s]\n",
      "  2%|▏         | 3014656/170498071 [00:55<38:21, 72767.25it/s]\n",
      "  2%|▏         | 3047424/170498071 [00:55<37:51, 73728.22it/s]\n",
      "  2%|▏         | 3080192/170498071 [00:56<45:32, 61262.38it/s]\n",
      "  2%|▏         | 3112960/170498071 [00:56<45:51, 60833.19it/s]\n",
      "2023-07-30 11:22:27,764\tINFO tune.py:1148 -- Total run time: 97.24 seconds (87.21 seconds for the tuning loop).\n",
      "2023-07-30 11:22:27,765\tWARNING tune.py:1163 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/luca/ray_results/train_cifar_2023-07-30_11-20-47\", trainable=...)\n",
      "2023-07-30 11:22:27,770\tWARNING experiment_analysis.py:916 -- Failed to read the results for 1 trials:\n",
      "- /home/luca/ray_results/train_cifar_2023-07-30_11-20-47/train_cifar_1572f_00000_0_batch_size=16,lr=0.0650_2023-07-30_11-20-50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'l1': 32, 'l2': 16, 'lr': 0.012647291777407418, 'batch_size': 16}\n",
      "Best trial final validation loss: 1.4740296248435973\n",
      "Best trial final validation accuracy: 0.4745\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 65536/170498071 [00:00<18:36, 152612.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 458752/170498071 [00:01<04:44, 596957.17it/s]"
     ]
    }
   ],
   "source": [
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
    "    config = {\n",
    "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "    }\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_cifar),\n",
    "            resources={\"cpu\": 2, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"loss\", \"min\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_result.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_result.metrics[\"accuracy\"]))\n",
    "\n",
    "    test_best_model(best_result)\n",
    "\n",
    "main(num_samples=2, max_num_epochs=2, gpus_per_trial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"#trials={len(result.trials)}\")\n",
    "print(f\"time={time.time()-start_time}\")\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"all\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "    best_trial.metric_analysis[\"loss\"][\"min\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(\n",
    "    best_trial.metric_analysis[\"accuracy\"][\"max\"]))\n",
    "\n",
    "best_trained_model = Net(2**best_trial.config[\"l1\"],\n",
    "                         2**best_trial.config[\"l2\"])\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if gpus_per_trial > 1:\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "best_trained_model.to(device)\n",
    "\n",
    "checkpoint_value = getattr(best_trial.checkpoint, \"dir_or_data\", None) or best_trial.checkpoint.value\n",
    "checkpoint_path = os.path.join(checkpoint_value, \"checkpoint\")\n",
    "\n",
    "model_state, optimizer_state = torch.load(checkpoint_path)\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "test_acc = _test_accuracy(best_trained_model, device)\n",
    "print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
