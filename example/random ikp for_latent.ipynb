{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ikflow/config.py | Using device: 'cuda:0'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f87b1f7c070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Optional\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from common.evaluate import evaluate_pose_error_J3d_P2d\n",
    "from paik.solver import NSF, PAIK, Solver, get_solver\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import BisectingKMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import jrl.robots as jrlib\n",
    "import paik.klampt_robot as chlib\n",
    "from ikp import get_robot, numerical_inverse_kinematics_batch, compute_mmd, gaussian_kernel, inverse_multiquadric_kernel, get_number_of_distinct_solutions\n",
    "\n",
    "import torch\n",
    "\n",
    "# set the same random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "from numpy import ndarray\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from tqdm.contrib import itertools as tqdm_itertools\n",
    "\n",
    "from paik.file import load_pickle, save_pickle\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, nsf: NSF):\n",
    "        \n",
    "        self.P_all = nsf.P\n",
    "        self.Z_all = nsf.Z\n",
    "        self.J_all = nsf.J\n",
    "        \n",
    "        self.temp_cluster_info = {}\n",
    "        \n",
    "    def init_cluster_info(self, max_samples_list: int, n_clusters_list: int):\n",
    "        print(\"Start to initialize cluster info...\")\n",
    "        for max_samples, n_clusters in tqdm_itertools.product(max_samples_list, n_clusters_list):\n",
    "            self.get_cluster_info(max_samples, n_clusters)\n",
    "    \n",
    "    def get_cluster_info(self, max_samples: int, n_clusters: int):\n",
    "        if f\"{max_samples}_{n_clusters}\" in self.temp_cluster_info:\n",
    "            return self.temp_cluster_info[f\"{max_samples}_{n_clusters}\"]['centroids_ids']\n",
    "\n",
    "        Z_samples = self.Z_all[:max_samples]\n",
    "        # buliding the clustering    \n",
    "        cluster = BisectingKMeans(n_clusters=n_clusters, random_state=0).fit(Z_samples)\n",
    "        centroids = cluster.cluster_centers_\n",
    "        # find centroids ids in Z_samples\n",
    "        Z_samples_knn = NearestNeighbors(n_neighbors=1).fit(Z_samples)\n",
    "        centroids_ids = Z_samples_knn.kneighbors(centroids, return_distance=False).flatten()\n",
    "        self.temp_cluster_info[f\"{max_samples}_{n_clusters}\"] = {\n",
    "            'centroids_ids': centroids_ids\n",
    "        }\n",
    "        return centroids_ids\n",
    "\n",
    "    def cluster_retriever(self, J: Optional[np.ndarray] = None, num_poses: int = 1, num_sols: int = 1, max_samples: int = 50000, radius: float = 0, n_clusters: int = 100):\n",
    "        Z_samples = self.Z_all[:max_samples]\n",
    "        J_samples = self.J_all[:max_samples]\n",
    "\n",
    "        centroids_ids = self.get_cluster_info(max_samples, n_clusters)\n",
    "        num_total = num_sols * num_poses\n",
    "\n",
    "        # diversity \n",
    "        if J is None:\n",
    "            ids = np.random.choice(centroids_ids, num_total, replace=True)\n",
    "        # selection\n",
    "        else:\n",
    "            # weight sampling based on the distance to the centroids\n",
    "            J_centroid = J_samples[centroids_ids] # shape: (num_centroids, nsf.n)\n",
    "\n",
    "            ids = np.empty((num_sols, num_poses), dtype=int)\n",
    "            for i in range(len(J)):\n",
    "                J_i = J[i]\n",
    "                dist = np.linalg.norm(J_centroid - J_i, axis=-1)\n",
    "                inv_dist = 1 / dist\n",
    "                # prob for each centroid based on the 1/dist\n",
    "                prob = inv_dist / inv_dist.sum()\n",
    "                ids[:, i] = np.random.choice(centroids_ids, num_sols, replace=True, p=prob)\n",
    "                # shuffle the ids\n",
    "                ids[:, i] = np.random.permutation(ids[:, i])\n",
    "            # shape: (num_sols, num_poses)  -> shape: (num_sols * num_poses)  \n",
    "            ids = ids.flatten()\n",
    "        Z_out = Z_samples[ids]\n",
    "        noise = np.random.normal(0, radius, size=Z_out.shape)\n",
    "        return Z_out + noise\n",
    "    \n",
    "    def random_retriever(self, J: Optional[np.ndarray] = None, num_poses: int = 1, num_sols: int = 1, max_samples: int = 1000, radius: float = 0):\n",
    "        print(\"Start to random retriever...\")\n",
    "        # diversity\n",
    "        if J is None:\n",
    "            ids = np.random.choice(max_samples, num_sols * num_poses, replace=True)\n",
    "        # selection\n",
    "        else:\n",
    "            J_knn = NearestNeighbors(n_neighbors=num_sols).fit(self.J_all[:max_samples])\n",
    "            ids = J_knn.kneighbors(J, return_distance=False)\n",
    "            # shape: (num_poses, num_sols) -> shape: (num_sols, num_poses)  -> shape: (num_sols * num_poses)  \n",
    "            ids = ids.T.flatten()\n",
    "        Z_out = self.Z_all[ids]\n",
    "        noise = np.random.normal(0, radius, size=Z_out.shape)\n",
    "        return Z_out + noise\n",
    "\n",
    "def solver_batch(solver, P, num_sols, std=0.001, retriever: Optional[Retriever] = None, J_ref=None, max_samples=50000, radius=0.0, num_clusters=30, verbose=False, use_cluster=True):\n",
    "    # shape: (num_sols, num_poses, m)\n",
    "    P_num_sols = np.expand_dims(P, axis=0).repeat(num_sols, axis=0)\n",
    "    # shape: (num_sols*num_poses, n)\n",
    "    P_num_sols = P_num_sols.reshape(-1, P.shape[-1])\n",
    "\n",
    "    if isinstance(solver, PAIK):\n",
    "        solver.base_std = std\n",
    "        F = solver.get_reference_partition_label(P=P, num_sols=num_sols)\n",
    "        # shape: (1, num_sols*num_poses, n)\n",
    "        J_hat = solver.generate_ik_solutions(P=P_num_sols, F=F, verbose=verbose)\n",
    "    elif isinstance(solver, NSF):\n",
    "        if retriever is None:\n",
    "            solver.base_std = std\n",
    "            J_hat = solver.generate_ik_solutions(P=P, num_sols=num_sols)\n",
    "        else:\n",
    "            if use_cluster:\n",
    "                latents = retriever.cluster_retriever(J=J_ref, num_poses=P.shape[0], num_sols=num_sols, max_samples=max_samples, radius=radius, n_clusters=num_clusters)\n",
    "            else:\n",
    "                latents = retriever.random_retriever(J=J_ref, num_poses=P.shape[0], num_sols=num_sols, max_samples=max_samples, radius=radius)\n",
    "            J_hat = solver.generate_ik_solutions(P=P_num_sols, latents=latents, verbose=verbose)\n",
    "    else:\n",
    "        J_hat = np.empty((num_sols, P.shape[0], solver.robot.n_dofs))\n",
    "        P_torch = torch.tensor(P, dtype=torch.float32).to('cuda')\n",
    "        for i, p in enumerate(P_torch):\n",
    "            solutions = solver.generate_ik_solutions(\n",
    "                p,\n",
    "                num_sols,\n",
    "                latent_distribution='gaussian',\n",
    "                latent_scale=std,\n",
    "                clamp_to_joint_limits=False,\n",
    "            )\n",
    "            J_hat[:, i] = solutions.detach().cpu().numpy()\n",
    "    # return shape: (num_sols, num_poses, n)\n",
    "    return J_hat.reshape(num_sols, P.shape[0], -1)\n",
    "\n",
    "\n",
    "def random_ikp(solver: Solver, P: np.ndarray, solve_fn_batch: Any, num_poses_list: np.ndarray, num_sols_list: np.ndarray, J_hat_num: Optional[np.ndarray] = None):\n",
    "    begin = time()\n",
    "    # shape: (num_poses, num_sols, num_dofs or n)\n",
    "    num_poses = P.shape[0]\n",
    "    num_sols = max(num_sols_list)\n",
    "    \n",
    "    J_hat = solve_fn_batch(P=P, num_sols=num_sols)\n",
    "    assert J_hat.shape == (\n",
    "        num_sols, num_poses, solver.robot.n_dofs), f\"J_hat shape {J_hat.shape} is not correct\"\n",
    "\n",
    "    l2, ang = evaluate_pose_error_J3d_P2d(\n",
    "        # input J.shape = (num_sols, num_poses, num_dofs or n)\n",
    "        solver.robot, J_hat, P, return_all=True\n",
    "    )\n",
    "    \n",
    "    l2 = l2.reshape(num_sols, num_poses)\n",
    "    ang = ang.reshape(num_sols, num_poses)\n",
    "    num_sols_time_ms = round((time() - begin) / len(P), 3) * 1000\n",
    "    \n",
    "    ret_results = {}\n",
    "    for num_sols, num_poses in itertools.product(num_sols_list, num_poses_list):\n",
    "        l2_mean = np.nanmean(l2[:num_sols, :num_poses])\n",
    "        ang_mean = np.nanmean(ang[:num_sols, :num_poses])\n",
    "        \n",
    "        ret_results[f'{num_poses}_{num_sols}'] = {\n",
    "            \"l2_mm\": l2_mean * 1000,\n",
    "            \"ang_deg\": np.rad2deg(ang_mean),\n",
    "            \"num_sols_time_ms\": num_sols_time_ms\n",
    "        }\n",
    "        \n",
    "        if J_hat_num is None:\n",
    "            mmd_guassian = np.nan\n",
    "            mmd_imq = np.nan\n",
    "        else:\n",
    "            mmd_guassian_list = np.empty((num_poses))\n",
    "            mmd_imq_list = np.empty((num_poses))\n",
    "            for i in range(num_poses):\n",
    "                mmd_guassian_list[i] = compute_mmd(J_hat[:num_sols, i], J_hat_num[:num_sols, i], kernel=gaussian_kernel)\n",
    "                mmd_imq_list[i] = compute_mmd(J_hat[:num_sols, i], J_hat_num[:num_sols, i], kernel=inverse_multiquadric_kernel)\n",
    "            mmd_guassian = mmd_guassian_list.mean()\n",
    "            mmd_imq = mmd_imq_list.mean()\n",
    "            \n",
    "        ret_results[f'{num_poses}_{num_sols}']['mmd_guassian'] = mmd_guassian\n",
    "        ret_results[f'{num_poses}_{num_sols}']['mmd_imq'] = mmd_imq\n",
    "\n",
    "    return J_hat, ret_results\n",
    "\n",
    "def nested_dict_to_2d_dict(nested_dict: dict):\n",
    "    ret_dict = {}\n",
    "    for key, value in nested_dict.items():\n",
    "        if isinstance(value, dict):\n",
    "            for k, v in value.items():\n",
    "                ret_dict[f\"{key}_{k}\"] = v\n",
    "        else:\n",
    "            ret_dict[key] = value\n",
    "    return ret_dict\n",
    "\n",
    "\n",
    "def random_ikp_with_mmd(record_dir, robot_name, num_poses_list, num_sols_list, paik_std_list, max_samples_list, radius_list, num_clusters_list):\n",
    "    robot = get_robot(robot_name)\n",
    "    nsf = get_solver(arch_name=\"nsf\", robot=robot, load=True, work_dir='/home/luca/paik')\n",
    "    retriever = Retriever(nsf)\n",
    "    paik = get_solver(arch_name=\"paik\", robot=robot, load=True, work_dir='/home/luca/paik')\n",
    "    \n",
    "    file_path = f\"{record_dir}/random_ikp_with_mmd_{robot_name}_{max(num_poses_list)}_{max(num_sols_list)}.pkl\"\n",
    "    \n",
    "    results = {}\n",
    "    if os.path.exists(file_path):\n",
    "        results = load_pickle(file_path)\n",
    "        ret_results = nested_dict_to_2d_dict(results)\n",
    "        df = pd.DataFrame(ret_results).T\n",
    "        # round to 4 decimal places\n",
    "        df = df.round(4)\n",
    "        print(df)\n",
    "        print(f\"Results are loaded from {file_path}\")\n",
    "    else:\n",
    "        print(f\"Results are not found in {file_path}\")\n",
    "        \n",
    "    if 'P' in results:\n",
    "        P = results['P']\n",
    "    else:\n",
    "        _, P = nsf.robot.sample_joint_angles_and_poses(n=max(num_poses_list))\n",
    "        \n",
    "    print(f\"Start numerical IK...\")\n",
    "    # num's variable: num_poses, num_sols\n",
    "    num_solver_batch = partial(numerical_inverse_kinematics_batch, solver=nsf)    \n",
    "    J_hat_num, results['num'] = random_ikp(nsf, P, num_solver_batch, num_poses_list, num_sols_list)\n",
    "    save_pickle(file_path, results)    \n",
    "    print(f\"Results numerical IK are saved in {file_path}\")\n",
    "    \n",
    "    print(f\"Start paik...\")\n",
    "    # paik's variable: num_poses, num_sols, std, \n",
    "    for std in tqdm(paik_std_list):\n",
    "        paik_solver_batch = partial(solver_batch, solver=paik, std=std)\n",
    "        name = f'paik_{std}_gaussian'\n",
    "        if name not in results:\n",
    "            _, results[name] = random_ikp(paik, P, paik_solver_batch, num_poses_list, num_sols_list, J_hat_num=J_hat_num)\n",
    "            save_pickle(file_path, results) \n",
    "    print(f\"Results paik are saved in {file_path}\")\n",
    "    \n",
    "    print(f\"Start nsf w/o retreiver...\")\n",
    "    # nsf's variable: std\n",
    "    for std in tqdm(paik_std_list):\n",
    "        nsf_solver_batch = partial(solver_batch, solver=nsf, std=std, retriever=None)\n",
    "        name = f'nsf_{std}_gaussian'\n",
    "        if name not in results:\n",
    "            _, results[name] = random_ikp(nsf, P, nsf_solver_batch, num_poses_list, num_sols_list, J_hat_num=J_hat_num)\n",
    "            save_pickle(file_path, results)\n",
    "\n",
    "    print(f\"Start nsf with cluster retriever...\")    \n",
    "    # nsf's variable: num_poses, num_sols, max_samples, radius, num_clusters\n",
    "    for max_samples, radius, num_clusters in tqdm_itertools.product(max_samples_list, radius_list, num_clusters_list):\n",
    "        nsf_solver_batch = partial(solver_batch, solver=nsf, max_samples=max_samples, radius=radius, num_clusters=num_clusters, retriever=retriever)\n",
    "        name = f'nsf_{max_samples}_{radius}_{num_clusters}_cluster'\n",
    "        if name not in results:\n",
    "            _, results[name] = random_ikp(nsf, P, nsf_solver_batch, num_poses_list, num_sols_list, J_hat_num=J_hat_num)\n",
    "            save_pickle(file_path, results)\n",
    "    print(f\"Results nsf with cluster retriever are saved in {file_path}\")\n",
    "    \n",
    "    print(f\"Start nsf with random retriever...\")\n",
    "    # nsf's variable: num_poses, num_sols, max_samples, radius\n",
    "    for max_samples, radius in tqdm_itertools.product(max_samples_list, radius_list):\n",
    "        nsf_solver_batch = partial(solver_batch, solver=nsf, max_samples=max_samples, radius=radius, retriever=retriever, use_cluster=False)\n",
    "        name = f'nsf_{max_samples}_{radius}_random'\n",
    "        if name not in results:\n",
    "            _, results[name] = random_ikp(nsf, P, nsf_solver_batch, num_poses_list, num_sols_list, J_hat_num=J_hat_num)\n",
    "            save_pickle(file_path, results)\n",
    "    \n",
    "    ret_results = nested_dict_to_2d_dict(results)\n",
    "\n",
    "    df = pd.DataFrame(ret_results).T\n",
    "    # round to 4 decimal places\n",
    "    df = df.round(4)\n",
    "    file_path = f\"{record_dir}/random_ikp_with_mmd_evaluation_results_{robot_name}_{max(num_poses_list)}_{max(num_sols_list)}.csv\"\n",
    "    df.to_csv(file_path)\n",
    "    print(f\"Results are saved in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to evaluate panda...\n",
      "WorldModel::LoadRobot: /home/luca/.cache/jrl/temp_urdfs/panda_arm_hand_formatted_link_filepaths_absolute.urdf\n",
      "joint mimic: no multiplier, using default value of 1 \n",
      "joint mimic: no offset, using default value of 0 \n",
      "URDFParser: Link size: 17\n",
      "URDFParser: Joint size: 12\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link0.dae (59388 verts, 20478 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link1.dae (37309 verts, 12516 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link2.dae (37892 verts, 12716 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link3.dae (42512 verts, 14233 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link4.dae (43520 verts, 14620 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link5.dae (54770 verts, 18327 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link6.dae (64086 verts, 21620 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link7.dae (35829 verts, 12077 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/hand.dae (20896 verts, 7078 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/finger.dae (1849 verts, 624 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/finger.dae (1849 verts, 624 tris)\n",
      "URDFParser: Done loading robot file /home/luca/.cache/jrl/temp_urdfs/panda_arm_hand_formatted_link_filepaths_absolute.urdf\n",
      "[Warning] Error(s) in loading state_dict for Flow:\n",
      "\tsize mismatch for base._0: copying a param with shape torch.Size([1, 7]) from checkpoint, the shape in current model is torch.Size([7]).. Please check the model path /home/luca/paik/weights/panda/0115-0234/model.pth.\n",
      "[WARNING] /home/luca/paik/weights/panda/0115-0234/J_knn.pth: file not exist and return None.. Load training data instead.\n",
      "[SUCCESS] P_knn load from /home/luca/paik/weights/panda/P_knn-5000000-7-7-1.pth.\n",
      "[SUCCESS] J_knn load from /home/luca/paik/weights/panda/J_knn-5000000-7-7-1.pth.\n",
      "[INFO] Load latent variable from /home/luca/paik/weights/panda/Z.npy.\n",
      "WorldModel::LoadRobot: /home/luca/.cache/jrl/temp_urdfs/panda_arm_hand_formatted_link_filepaths_absolute.urdf\n",
      "joint mimic: no multiplier, using default value of 1 \n",
      "joint mimic: no offset, using default value of 0 \n",
      "URDFParser: Link size: 17\n",
      "URDFParser: Joint size: 12\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link0.dae (59388 verts, 20478 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link1.dae (37309 verts, 12516 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link2.dae (37892 verts, 12716 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link3.dae (42512 verts, 14233 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link4.dae (43520 verts, 14620 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link5.dae (54770 verts, 18327 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link6.dae (64086 verts, 21620 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link7.dae (35829 verts, 12077 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/hand.dae (20896 verts, 7078 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/finger.dae (1849 verts, 624 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/finger.dae (1849 verts, 624 tris)\n",
      "URDFParser: Done loading robot file /home/luca/.cache/jrl/temp_urdfs/panda_arm_hand_formatted_link_filepaths_absolute.urdf\n",
      "[SUCCESS] load from /home/luca/paik/weights/panda/0904-1939\n",
      "[SUCCESS] load best date 0904-1939 with l2 0.00297 from /home/luca/paik/weights/panda/best_date_paik.csv.\n",
      "                                         l2_mm  ang_deg  num_sols_time_ms  \\\n",
      "num_1000_1000                           0.4222   0.0517             535.0   \n",
      "paik_0.001_gaussian_1000_1000           3.9967   2.3483              73.0   \n",
      "paik_0.1_gaussian_1000_1000             4.0637   2.3867              74.0   \n",
      "paik_0.25_gaussian_1000_1000            4.1664   2.4390              74.0   \n",
      "paik_0.5_gaussian_1000_1000             4.6991   2.6983              74.0   \n",
      "paik_0.7_gaussian_1000_1000             5.4719   3.0565              73.0   \n",
      "nsf_0.001_gaussian_1000_1000            2.8901   2.0413              73.0   \n",
      "nsf_0.1_gaussian_1000_1000              2.9745   1.8645              74.0   \n",
      "nsf_0.25_gaussian_1000_1000             3.2137   1.9092              73.0   \n",
      "nsf_0.5_gaussian_1000_1000              3.7233   2.0861              78.0   \n",
      "nsf_0.7_gaussian_1000_1000              4.4432   2.4039              73.0   \n",
      "nsf_5000000_0.001_13_cluster_1000_1000  3.3942   1.9203              96.0   \n",
      "nsf_5000000_0.001_19_cluster_1000_1000  3.4278   1.9514              96.0   \n",
      "nsf_5000000_0.001_25_cluster_1000_1000  3.3958   2.0026              97.0   \n",
      "nsf_5000000_0.001_30_cluster_1000_1000  3.4661   1.9942              96.0   \n",
      "nsf_5000000_0.001_40_cluster_1000_1000  3.5502   2.0291              97.0   \n",
      "nsf_5000000_0.1_13_cluster_1000_1000    3.4367   1.9307              74.0   \n",
      "nsf_5000000_0.1_19_cluster_1000_1000    3.4783   1.9539              74.0   \n",
      "nsf_5000000_0.1_25_cluster_1000_1000    3.4153   1.9898              74.0   \n",
      "nsf_5000000_0.1_30_cluster_1000_1000    3.4715   2.0157              74.0   \n",
      "nsf_5000000_0.1_40_cluster_1000_1000    3.5503   2.0278              74.0   \n",
      "nsf_5000000_0.25_13_cluster_1000_1000   3.5452   1.9925              74.0   \n",
      "nsf_5000000_0.25_19_cluster_1000_1000   3.5974   2.0030              74.0   \n",
      "nsf_5000000_0.25_25_cluster_1000_1000   3.5555   2.0391              74.0   \n",
      "nsf_5000000_0.25_30_cluster_1000_1000   3.5916   2.0557              74.0   \n",
      "\n",
      "                                        mmd_guassian  mmd_imq  \n",
      "num_1000_1000                                    NaN      NaN  \n",
      "paik_0.001_gaussian_1000_1000                 0.1096   0.0842  \n",
      "paik_0.1_gaussian_1000_1000                   0.0963   0.0737  \n",
      "paik_0.25_gaussian_1000_1000                  0.0782   0.0602  \n",
      "paik_0.5_gaussian_1000_1000                   0.0578   0.0455  \n",
      "paik_0.7_gaussian_1000_1000                   0.0484   0.0386  \n",
      "nsf_0.001_gaussian_1000_1000                  0.8618   0.6399  \n",
      "nsf_0.1_gaussian_1000_1000                    0.4075   0.2958  \n",
      "nsf_0.25_gaussian_1000_1000                   0.1540   0.1151  \n",
      "nsf_0.5_gaussian_1000_1000                    0.0568   0.0439  \n",
      "nsf_0.7_gaussian_1000_1000                    0.0436   0.0341  \n",
      "nsf_5000000_0.001_13_cluster_1000_1000        0.1182   0.0889  \n",
      "nsf_5000000_0.001_19_cluster_1000_1000        0.1057   0.0800  \n",
      "nsf_5000000_0.001_25_cluster_1000_1000        0.0815   0.0628  \n",
      "nsf_5000000_0.001_30_cluster_1000_1000        0.0943   0.0715  \n",
      "nsf_5000000_0.001_40_cluster_1000_1000        0.0790   0.0610  \n",
      "nsf_5000000_0.1_13_cluster_1000_1000          0.0922   0.0692  \n",
      "nsf_5000000_0.1_19_cluster_1000_1000          0.0856   0.0649  \n",
      "nsf_5000000_0.1_25_cluster_1000_1000          0.0696   0.0533  \n",
      "nsf_5000000_0.1_30_cluster_1000_1000          0.0802   0.0607  \n",
      "nsf_5000000_0.1_40_cluster_1000_1000          0.0682   0.0524  \n",
      "nsf_5000000_0.25_13_cluster_1000_1000         0.0659   0.0506  \n",
      "nsf_5000000_0.25_19_cluster_1000_1000         0.0641   0.0494  \n",
      "nsf_5000000_0.25_25_cluster_1000_1000         0.0573   0.0442  \n",
      "nsf_5000000_0.25_30_cluster_1000_1000         0.0608   0.0466  \n",
      "Results are loaded from /mnt/d/pads/Documents/paik_store/record/2024_11_06/random_ikp_with_mmd_panda_1000_1000.pkl\n",
      "Start numerical IK...\n",
      "Results numerical IK are saved in /mnt/d/pads/Documents/paik_store/record/2024_11_06/random_ikp_with_mmd_panda_1000_1000.pkl\n",
      "Start paik...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 46707.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results paik are saved in /mnt/d/pads/Documents/paik_store/record/2024_11_06/random_ikp_with_mmd_panda_1000_1000.pkl\n",
      "Start nsf w/o retreiver...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 48099.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start nsf with cluster retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007646322250366211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 8,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7651397201475db365a3ba5a293870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results nsf with cluster retriever are saved in /mnt/d/pads/Documents/paik_store/record/2024_11_06/random_ikp_with_mmd_panda_1000_1000.pkl\n",
      "Start nsf with random retriever...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006794929504394531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b65772d7cc46df8cec6f3ca49f1f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to random retriever...\n",
      "Start to random retriever...\n",
      "Start to random retriever...\n",
      "Start to random retriever...\n",
      "Results are saved in /mnt/d/pads/Documents/paik_store/record/2024_11_06/random_ikp_with_mmd_evaluation_results_panda_1000_1000.csv\n"
     ]
    }
   ],
   "source": [
    "from common.config import Config_IKP\n",
    "config = Config_IKP()\n",
    "\n",
    "config.workdir = '/mnt/d/pads/Documents/paik_store'\n",
    "\n",
    "kwarg = {\n",
    "    'record_dir': config.record_dir,\n",
    "    'robot_name': 'panda',\n",
    "    'num_poses_list': [1000], # 300, 500, 1000\n",
    "    'num_sols_list': [1000],  # 300, 500, 1000\n",
    "    'paik_std_list': [0.001, 0.1, 0.25, 0.5, 0.7], # 0.001, 0.1, 0.25, 0.5, 0.7\n",
    "    'max_samples_list': np.array([5e6], int), # 1e5, 1e6, 2e6\n",
    "    'radius_list': [0.001, 0.1, 0.25, 0.5], # 0, 0.1, 0.3, 0.5, 0.7, 0.9\n",
    "    'num_clusters_list': [16, 25] # 13, 16, 19, 25, 30, 40\n",
    "}\n",
    "\n",
    "robot_names = [\"panda\"] # \"panda\", \"fetch\", \"fetch_arm\", \"atlas_arm\", \"atlas_waist_arm\", \"baxter_arm\"\n",
    "\n",
    "for robot_name in robot_names:\n",
    "    print(f\"Start to evaluate {robot_name}...\")\n",
    "    kwarg['robot_name'] = robot_name\n",
    "    random_ikp_with_mmd(**kwarg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(f'{config.record_dir}/random_ikp_with_mmd_evaluation_results_panda_1000_1000.csv', index_col=0)\n",
    "mi = df.index.str.split('_', expand=True)\n",
    "df_mi = df.set_index(mi)\n",
    "# set index names\n",
    "df_mi.index.names = ['solver', 'max_samples', 'radius', 'num_clusters', 'num_poses', 'num_sols']\n",
    "df_mi.reset_index(inplace=True)\n",
    "\n",
    "# swap values in the columns of max_samples and num_poses for solver num\n",
    "df_mi.loc[df_mi.solver == 'num', ['max_samples', 'num_poses']] = df_mi.loc[df_mi.solver == 'num', ['num_poses', 'max_samples']].values\n",
    "# swap values in the columns of radius and num_sols for solver num\n",
    "df_mi.loc[df_mi.solver == 'num', ['radius', 'num_sols']] = df_mi.loc[df_mi.solver == 'num', ['num_sols', 'radius']].values\n",
    "\n",
    "# set new columns as float \n",
    "df_mi[['max_samples', 'radius', 'num_clusters', 'num_poses', 'num_sols']] = df_mi[['max_samples', 'radius', 'num_clusters', 'num_poses', 'num_sols']].astype(float)\n",
    "df_mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter nsf\n",
    "df_nsf = df_mi[df_mi.solver == 'nsf']\n",
    "# nsf has variables: max_samples, radius, num_clusters\n",
    "# fix the other variables, e.g. num_poses, num_sols.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_nsf_linechart(df, hue, x_label, y_labels=['l2_mm', 'mmd_imq']):\n",
    "    df_cp = df.copy()\n",
    "    df_cp = df_cp.sort_values(x_label)\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    for i, y_label in enumerate(y_labels):\n",
    "        sns.lineplot(data=df_cp, x=x_label, y=y_label, hue=hue, marker='o', ax=axs[i])\n",
    "        y_label = y_label.replace('_', ' ').upper()\n",
    "        axs[i].set_title(f'NSF {y_label} vs {x_label.replace(\"_\", \" \").title()}')\n",
    "        axs[i].set_xlabel(f'{x_label.replace(\"_\", \" \").title()}')\n",
    "        axs[i].set_ylabel(y_label)\n",
    "        axs[i].grid()\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "# plot the linechart where x-axis is max_samples, y-axis is mmd_imq\n",
    "# and each line is a different num_clusters. \n",
    "# Fix radius=0, num_poses=1000, num_sols=1000\n",
    "df_cp = df_nsf.copy()\n",
    "# select the rows where nnum_poses=1000, num_sols=1000, radius=0\n",
    "df_cp = df_cp[(df_cp.num_poses == 1000) & (df_cp.num_sols == 1000) & (df_cp.radius == 0)]\n",
    "plot_nsf_linechart(df_cp, hue='num_clusters', x_label='max_samples')\n",
    "\n",
    "# plot the linechart where x-axis is radius, y-axis is mmd_imq\n",
    "# and each line is a different num_clusters. \n",
    "# Fix max_samples=5000000, num_poses=1000, num_sols=1000.\n",
    "df_cp = df_nsf.copy()\n",
    "# select the rows where nnum_poses=1000, num_sols=1000, max_samples=5000000\n",
    "df_cp = df_cp[(df_cp.num_poses == 1000) & (df_cp.num_sols == 1000) & (df_cp.max_samples == 5000000)]\n",
    "plot_nsf_linechart(df_cp, hue='num_clusters', x_label='radius')\n",
    "\n",
    "# plot the linechart where x-axis is max_samples, y-axis is mmd_imq\n",
    "# and each line is a different radius.\n",
    "# Fix num_clusters=25, num_poses=1000, num_sols=1000.\n",
    "df_cp = df_nsf.copy()\n",
    "# select the rows where nnum_poses=1000, num_sols=1000, num_clusters=25\n",
    "df_cp = df_cp[(df_cp.num_poses == 1000) & (df_cp.num_sols == 1000) & (df_cp.num_clusters == 25)]\n",
    "plot_nsf_linechart(df_cp, hue='radius', x_label='max_samples')\n",
    "\n",
    "# plot the linechart where x-axis is num_poses, y-axis is mmd_imq\n",
    "# and each line is a different num_sols.\n",
    "# Fix max_samples=5000000, radius=0.5, num_clusters=25.\n",
    "df_cp = df_nsf.copy()\n",
    "# select the rows where max_samples=5000000, radius=0.5, num_clusters=25\n",
    "df_cp = df_cp[(df_cp.max_samples == 5000000) & (df_cp.radius == 0.5) & (df_cp.num_clusters == 25)]\n",
    "plot_nsf_linechart(df_cp, hue='num_sols', x_label='num_poses')\n",
    "\n",
    "# plot two sub-plot linechart where x-axis are max_samples, one y-axis is l2_mm and one is mmd_imq\n",
    "# and each line is a different num_clusters. \n",
    "# Fix radius=0, num_poses=1000, num_sols=1000.\n",
    "df_cp = df_nsf.copy()\n",
    "# select the rows where nnum_poses=1000, num_sols=1000, radius=0\n",
    "df_cp = df_cp[(df_cp.num_poses == 1000) & (df_cp.num_sols == 1000) & (df_cp.radius == 0)]\n",
    "plot_nsf_linechart(df_cp, hue='num_clusters', x_label='max_samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mi.loc['paik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flatdict import FlatterDict\n",
    "\n",
    "nested_dict = {'a': 1, 'c': {'a': 2, 'b': {'x': 3, 'y': 4, 'z': 5}}, 'd': [6, 7, 8]}\n",
    "flat_dict = FlatterDict(nested_dict, delimiter='_')\n",
    "print(dict(flat_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show method == random\n",
    "df[df[\"method\"] == \"random\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "\n",
    "robot_name = 'panda'\n",
    "\n",
    "max_samples_list = [10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000]\n",
    "radius_list = [0.0, 0.01, 0.1, 0.5, 1.0, 2]\n",
    "j_ref_list = [None]\n",
    "use_cluster_list = [False, True]\n",
    "n_clusters_list = [5, 10, 20, 30, 50, 100, 200]\n",
    "\n",
    "# Combine as an iterator\n",
    "combinations = itertools.product(max_samples_list, radius_list, j_ref_list, use_cluster_list, n_clusters_list)\n",
    "\n",
    "for com in combinations:\n",
    "    \n",
    "    MAX_SAMPLES, RADIUS, J_REF, USE_CLUSTER, N_CLUSTERS = com\n",
    "    print_retriever()\n",
    "    \n",
    "    if not USE_CLUSTER and N_CLUSTERS > n_clusters_list[0]:\n",
    "        continue\n",
    "    \n",
    "    if USE_CLUSTER and MAX_SAMPLES < N_CLUSTERS * 1000:\n",
    "        continue\n",
    "    \n",
    "    record_dir = f'/home/luca/paik/record/retriever/'\n",
    "    \n",
    "    if USE_CLUSTER:\n",
    "        record_dir += 'cluster'\n",
    "    else:\n",
    "        record_dir += 'random'\n",
    "    \n",
    "    record_dir += f'_sam{MAX_SAMPLES}_r{RADIUS}_clstr{N_CLUSTERS}'\n",
    "    \n",
    "    os.makedirs(record_dir, exist_ok=True)\n",
    "    test_random_ikp_with_mmd(robot_name, \"diag_normal\", 150, 150, [0.01], record_dir, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the results from the record directory\n",
    "combinations = itertools.product(max_samples_list, radius_list, j_ref_list, use_cluster_list, n_clusters_list)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for com in combinations:\n",
    "    \n",
    "    MAX_SAMPLES, RADIUS, J_REF, USE_CLUSTER, N_CLUSTERS = com\n",
    "    \n",
    "    record_dir = f'/home/luca/paik/record/retriever/'\n",
    "    \n",
    "    if USE_CLUSTER:\n",
    "        record_dir += 'cluster'\n",
    "    else:\n",
    "        record_dir += 'random'\n",
    "    \n",
    "    record_dir += f'_sam{MAX_SAMPLES}_r{RADIUS}_clstr{N_CLUSTERS}'\n",
    "\n",
    "    if not os.path.exists(record_dir):\n",
    "        continue\n",
    "    \n",
    "    df_file = pd.read_csv(f\"{record_dir}/ikp_{robot_name}_150_150_0.01_nsf_diag_normal.csv\")\n",
    "    # convert to pd series with mean of df columns as keys\n",
    "    # add max_samples, radius, j_ref, use_cluster, n_clusters as keys\n",
    "    series = df_file.mean(numeric_only=True)\n",
    "    series[\"max_samples\"] = MAX_SAMPLES\n",
    "    series[\"radius\"] = RADIUS\n",
    "    series[\"j_ref\"] = False if J_REF is None else True\n",
    "    series[\"method\"] = \"cluster\" if USE_CLUSTER else \"random\"\n",
    "    series[\"n_clusters\"] = N_CLUSTERS\n",
    "\n",
    "    df_list.append(series)\n",
    "\n",
    "df = pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show method == random\n",
    "df[df[\"method\"] == \"random\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster's variables: max_samples, radius, n_clusters\n",
    "\n",
    "# lineplots for cluster method with radius = 0.0.\n",
    "# x_axis is different max_samples \n",
    "# y_axis is mmd_imq\n",
    "# lines are different number of clusters\n",
    "df_cluster = df[(df[\"method\"] == \"cluster\") & (df[\"radius\"] == 0.0)]\n",
    "df_cluster = df_cluster.sort_values(by=\"n_clusters\")\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_cluster, x=\"max_samples\", y=\"mmd_imq\", hue=\"n_clusters\", marker=\"o\")\n",
    "# set x_ticks as max_samples_list\n",
    "plt.xticks(max_samples_list)\n",
    "plt.title(\"MMD_IMQ with different number of clusters\")\n",
    "plt.show()\n",
    "\n",
    "# n_clsuter domainates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster's variables: max_samples, radius, n_clusters\n",
    "\n",
    "# lineplots for cluster method with max_samples = 50000.\n",
    "# x_axis is different raius \n",
    "# y_axis is mmd_imq\n",
    "# lines are different number of clusters\n",
    "df_cluster = df[(df[\"method\"] == \"cluster\") & (df[\"max_samples\"] == 100000)]\n",
    "df_cluster = df_cluster.sort_values(by=\"radius\")\n",
    "\n",
    "# print out the row of the min mmd_imq\n",
    "print(df_cluster[df_cluster[\"mmd_imq\"] == df_cluster[\"mmd_imq\"].min()].T)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_cluster, x=\"radius\", y=\"mmd_imq\", hue=\"n_clusters\", marker=\"o\")\n",
    "# set x_ticks as radius_list\n",
    "plt.xticks(radius_list)\n",
    "plt.title(\"Cluster's MMD_IMQ with different radius\")\n",
    "plt.show()\n",
    "\n",
    "# radius has a balance near 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random's variables: max_samples, radius \n",
    "\n",
    "# lineplots for random method. \n",
    "# x_axis is different radius\n",
    "# y_axis is mmd_imq\n",
    "# lines are different max_samples\n",
    "\n",
    "df_random = df[df[\"method\"] == \"random\"]\n",
    "df_random = df_random.sort_values(by=\"max_samples\")\n",
    "\n",
    "print(df_random[df_random[\"mmd_imq\"] == df_random[\"mmd_imq\"].min()].T)\n",
    "\n",
    "df_random_first_5 = df_random[df_random[\"max_samples\"] <= 1000]\n",
    "df_random_rest = df_random[df_random[\"max_samples\"] > 1000]\n",
    "\n",
    "# plot the first 5 max_samples as a sub-plot and the rest as a sub-plot\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
    "sns.lineplot(data=df_random_first_5, x=\"radius\", y=\"mmd_imq\", hue=\"max_samples\", marker=\"o\", ax=axes[0])\n",
    "sns.lineplot(data=df_random_rest, x=\"radius\", y=\"mmd_imq\", hue=\"max_samples\", marker=\"o\", ax=axes[1])\n",
    "plt.xticks(radius_list)\n",
    "plt.title(\"MMD_IMQ with different radius\")\n",
    "plt.show()\n",
    "\n",
    "# radius has a balance near 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
