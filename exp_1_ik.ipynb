{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from os import path \n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm, trange\n",
    "import zuko\n",
    "from zuko.flows import Distribution, NSF\n",
    "from zuko.distributions import DiagNormal, BoxUniform, Minimum\n",
    "from zuko.flows import DistributionModule, FlowModule, Unconditional\n",
    "from hnne import HNNE\n",
    "\n",
    "from utils.settings import config\n",
    "from utils.utils import *\n",
    "from utils.model import *\n",
    "from utils.robot import Robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hnne load successfully from /home/luca/ikpflow/weights/panda/hnne.pickle\n",
      "Model load successfully from /home/luca/ikpflow/weights/panda/nsf.pth\n"
     ]
    }
   ],
   "source": [
    "panda = Robot(verbose=False)\n",
    "# data generation\n",
    "J, P = data_collection(robot=panda, N=config.num_train_size)\n",
    "# build dimension reduction model\n",
    "F = posture_feature_extraction(J)\n",
    "# hnne = get_hnne_model(J, P)\n",
    "# get loader\n",
    "loader = get_train_loader(J, P, F)\n",
    "# loader = get_loader(J, P, hnne=hnne)\n",
    "# get val loader\n",
    "J, P = data_collection(robot=panda, N=config.num_val_size)\n",
    "val_loader = get_test_loader(P, F)\n",
    "# Build Generative model, NSF\n",
    "# Neural spline flow (NSF) with 3 sample features and 5 context features\n",
    "flow, optimizer, scheduler = get_flow_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 7]), torch.Size([128, 7]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Jb, Cb = next(iter(val_loader))\n",
    "Jb.shape, Cb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.zeros(size=(7,))\n",
    "NUM_DATA = 100\n",
    "NUM_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l2_err</th>\n",
       "      <th>time_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.012720</td>\n",
       "      <td>0.08240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.045122</td>\n",
       "      <td>0.02196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.006217</td>\n",
       "      <td>0.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.324739</td>\n",
       "      <td>0.30000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              l2_err     time_diff\n",
       "count  100000.000000  100000.00000\n",
       "mean        0.012720       0.08240\n",
       "std         0.045122       0.02196\n",
       "min         0.000009       0.08000\n",
       "25%         0.002278       0.08000\n",
       "50%         0.003694       0.08000\n",
       "75%         0.006217       0.08000\n",
       "max         1.324739       0.30000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, err = test_l2_err(robot=panda, \n",
    "                      loader=val_loader, \n",
    "                      model=flow, \n",
    "                      num_data=NUM_DATA, \n",
    "                      num_samples=NUM_SAMPLES)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l2_err</th>\n",
       "      <th>time_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.008768</td>\n",
       "      <td>0.08240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.036756</td>\n",
       "      <td>0.02196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.294425</td>\n",
       "      <td>0.30000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              l2_err     time_diff\n",
       "count  100000.000000  100000.00000\n",
       "mean        0.008768       0.08240\n",
       "std         0.036756       0.02196\n",
       "min         0.000040       0.08000\n",
       "25%         0.001438       0.08000\n",
       "50%         0.002349       0.08000\n",
       "75%         0.003854       0.08000\n",
       "max         1.294425       0.30000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iflow = get_iflow_model(flow, init_sample=mu, shrink_ratio=0.31)\n",
    "df, err = test_l2_err(robot=panda, \n",
    "                      loader=val_loader, \n",
    "                      model=iflow, \n",
    "                      num_data=NUM_DATA, \n",
    "                      num_samples=NUM_SAMPLES)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l2_err</th>\n",
       "      <th>time_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.003576</td>\n",
       "      <td>0.0802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.011440</td>\n",
       "      <td>0.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.003237</td>\n",
       "      <td>0.0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.586376</td>\n",
       "      <td>0.0900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              l2_err    time_diff\n",
       "count  100000.000000  100000.0000\n",
       "mean        0.003576       0.0802\n",
       "std         0.011440       0.0014\n",
       "min         0.000038       0.0800\n",
       "25%         0.001303       0.0800\n",
       "50%         0.002094       0.0800\n",
       "75%         0.003237       0.0800\n",
       "max         0.586376       0.0900"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nflow = get_nflow_model(flow=flow)\n",
    "df, err = test_l2_err(robot=panda, \n",
    "                      loader=val_loader, \n",
    "                      model=nflow, \n",
    "                      num_data=NUM_DATA, \n",
    "                      num_samples=NUM_SAMPLES)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ikflow.utils import set_seed\n",
    "from ikflow.model_loading import get_ik_solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_seed() - random int:  44\n",
      "WorldModel::LoadRobot: /tmp/panda_arm_hand_formatted_link_filepaths_absolute.urdf\n",
      "ndim_tot=7\n",
      "dim_cond=8\n",
      "joint mimic: no multiplier, using default value of 1 \n",
      "joint mimic: no offset, using default value of 0 \n",
      "URDFParser: Link size: 17\n",
      "URDFParser: Joint size: 12\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jkinpylib/urdfs/panda/meshes/collision/link0.stl (595 verts, 200 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jkinpylib/urdfs/panda/meshes/collision/link1.stl (887 verts, 300 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jkinpylib/urdfs/panda/meshes/collision/link2.stl (889 verts, 300 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jkinpylib/urdfs/panda/meshes/collision/link3.stl (900 verts, 300 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jkinpylib/urdfs/panda/meshes/collision/link4.stl (900 verts, 300 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jkinpylib/urdfs/panda/meshes/collision/link5.stl (900 verts, 300 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jkinpylib/urdfs/panda/meshes/collision/link6.stl (600 verts, 200 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jkinpylib/urdfs/panda/meshes/collision/link7.stl (600 verts, 200 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jkinpylib/urdfs/panda/meshes/collision/hand.stl (600 verts, 200 tris)\n",
      "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jkinpylib/urdfs/panda/meshes/collision/finger.stl (96 verts, 32 tris)\n",
      "URDFParser: Done loading robot file /tmp/panda_arm_hand_formatted_link_filepaths_absolute.urdf\n"
     ]
    }
   ],
   "source": [
    "file_names = ['ang_errs_avg', 'ang_errs_min', 'F_avg', 'F_min', 'ikflow_ang', 'ikflow_l2', 'l2_errs_avg', 'l2_errs_min', 'n_evals']\n",
    "exp_5_fig_dir = config.traj_dir + f'figs/exp_5_{datetime.now().strftime(\"%m%d%H%M\")}/'\n",
    "if not os.path.exists(path=exp_5_fig_dir):\n",
    "    os.makedirs(exp_5_fig_dir)\n",
    "set_seed()\n",
    "num_trails = 100\n",
    "num_generation = 100\n",
    "num_ikflow_trails = num_generation\n",
    "num_solutions = 500\n",
    "# Build IKFlowSolver and set weights\n",
    "ik_solver, hyper_parameters = get_ik_solver(\"panda__full__lp191_5.25m\")\n",
    "robot = ik_solver.robot\n",
    "panda = Robot(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_poses(loader):\n",
    "    joint, ee = next(iter(loader))\n",
    "    ee = ee.cpu().numpy()\n",
    "    quaternions = np.zeros((len(ee), 4))\n",
    "    quaternions[:, 0] = np.random.randn() * 2e-2 + 1\n",
    "    target_poses = np.column_stack((ee[:, :3], quaternions))\n",
    "    return target_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_poses = get_target_poses(val_loader)\n",
    "target_poses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
      "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n"
     ]
    }
   ],
   "source": [
    "# -> unrefined solutions\n",
    "l2_errs = np.zeros((len(target_poses), NUM_SAMPLES))\n",
    "time_diffs = np.zeros((len(target_poses), NUM_SAMPLES))\n",
    "\n",
    "for i, target_pose in enumerate(target_poses):\n",
    "    iksols, l2_err, _, _, _, dt = ik_solver.solve(\n",
    "        target_pose, \n",
    "        n=NUM_SAMPLES,\n",
    "        refine_solutions=False, \n",
    "        return_detailed=True)\n",
    "\n",
    "    iksols = solutions.detach().cpu().numpy()\n",
    "    dt = np.zeros_like(l2_err) + dt\n",
    "    \n",
    "    l2_errs[i] = l2_err\n",
    "    time_diffs[i] = dt\n",
    "\n",
    "l2_errs = l2_errs.reshape((-1))\n",
    "time_diffs = time_diffs.reshape((-1))\n",
    "\n",
    "    \n",
    "df = pd.DataFrame(\n",
    "    data=np.column_stack((l2_errs, time_diffs)),\n",
    "    columns=[\"l2_err\", \"time_diff\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l2_err</th>\n",
       "      <th>time_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>128000.000000</td>\n",
       "      <td>128000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.090654</td>\n",
       "      <td>0.201037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.227391</td>\n",
       "      <td>0.021056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.162164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.191162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.201805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.052177</td>\n",
       "      <td>0.208315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.682320</td>\n",
       "      <td>0.343473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              l2_err      time_diff\n",
       "count  128000.000000  128000.000000\n",
       "mean        0.090654       0.201037\n",
       "std         0.227391       0.021056\n",
       "min         0.000068       0.162164\n",
       "25%         0.003930       0.191162\n",
       "50%         0.006380       0.201805\n",
       "75%         0.052177       0.208315\n",
       "max         1.682320       0.343473"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
