{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required packages\n",
                "import torch\n",
                "import os\n",
                "import numpy as np\n",
                "\n",
                "from utils.solver import Solver, DEFAULT_SOLVER_PARAM_M7\n",
                "from utils.robot import get_robot, sample_P_path, sample_J_traj\n",
                "from utils.utils import load_numpy, save_numpy, nearest_neighbor_F"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WorldModel::LoadRobot: /home/luca/.cache/jrl/temp_urdfs/panda_arm_hand_formatted_link_filepaths_absolute.urdf\n",
                        "joint mimic: no multiplier, using default value of 1 \n",
                        "joint mimic: no offset, using default value of 0 \n",
                        "URDFParser: Link size: 17\n",
                        "URDFParser: Joint size: 12\n",
                        "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link0.dae (59388 verts, 20478 tris)\n",
                        "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link1.dae (37309 verts, 12516 tris)\n",
                        "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link2.dae (37892 verts, 12716 tris)\n",
                        "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link3.dae (42512 verts, 14233 tris)\n",
                        "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link4.dae (43520 verts, 14620 tris)\n",
                        "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link5.dae (54770 verts, 18327 tris)\n",
                        "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link6.dae (64086 verts, 21620 tris)\n",
                        "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/link7.dae (35829 verts, 12077 tris)\n",
                        "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/hand.dae (20896 verts, 7078 tris)\n",
                        "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/finger.dae (1849 verts, 624 tris)\n",
                        "LoadAssimp: Loaded model /home/luca/miniconda3/lib/python3.9/site-packages/jrl/urdfs/panda/meshes/visual/finger.dae (1849 verts, 624 tris)\n",
                        "URDFParser: Done loading robot file /home/luca/.cache/jrl/temp_urdfs/panda_arm_hand_formatted_link_filepaths_absolute.urdf\n",
                        "Initialized robot collision data structures in time 0.43703\n",
                        "Model load successfully from /home/luca/ikpflow/weights/panda/1018-0133.pth\n",
                        "F load successfully from /home/luca/ikpflow/data/panda/train/F-2400000-7-7-1.npy\n",
                        "knn load successfully from /home/luca/ikpflow/data/panda/train/knn-2400000-7-7-1-normFalse.pickle\n"
                    ]
                }
            ],
            "source": [
                "robot = get_robot()\n",
                "solver = Solver(robot=robot, solver_param=DEFAULT_SOLVER_PARAM_M7)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_poses = 100\n",
                "num_sols = 1000"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "(0.013211406473133885, 0.14541286412000656, 0.153)"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# solver.latent = torch.zeros(1, 7).cuda()\n",
                "# solver.shirnk_ratio = 0.31\n",
                "solver.random_evaluation(num_poses, num_sols, return_time=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n",
                        "Heads up: It may be faster to run solution_pose_errors() with pytorch directly on the cpu/gpu\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "(0.009371630721162398, 0.09595398182630539, 0.155)"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# solver.latent = torch.zeros(1, 7).cuda()\n",
                "solver.shirnk_ratio = 0.31\n",
                "solver.random_evaluation(num_poses, num_sols, return_time=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Path_dir = sample_P_path(load_time='', num_steps=20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def path_following(\n",
                "    solver,\n",
                "    Path_dir: str,\n",
                "    num_traj: int = 3,\n",
                ") -> None:\n",
                "    \"\"\"\n",
                "    path following generation for our method\n",
                "    \n",
                "    Parameters\n",
                "    ----------\n",
                "    robot : _type_\n",
                "        robotic arm\n",
                "    Path_dir : str\n",
                "        path to ee Path, generated by sample_P_path\n",
                "    model : _type_\n",
                "        flow, iflow, or nflow\n",
                "    knn : _type_\n",
                "        knn of P_train\n",
                "    F : _type_\n",
                "        F_train\n",
                "    num_traj : int, optional\n",
                "        the number of generated joint trajectory samples, by default 3\n",
                "    \"\"\"\n",
                "    Path = load_numpy(file_path=Path_dir + \"ee_traj.npy\")\n",
                "    # Path = Path[:, :3]\n",
                "\n",
                "    ref_F = nearest_neighbor_F(knn, np.atleast_2d(Path[0]), F, n_neighbors=40_0000) # knn\n",
                "    # ref_F = F\n",
                "    # ref_F = rand_F(Path[0], F)\n",
                "    \n",
                "    exp_path = lambda idx: Path_dir + f\"exp_{idx}.npy\"\n",
                "    \n",
                "    # rand_idxs = np.random.randint(low=0, high=len(ref_F), size=num_traj)\n",
                "    rand_idxs = [0, 0, 0]\n",
                "\n",
                "    # rand_idxs = list(range(num_traj))\n",
                "    for i, rand in enumerate(rand_idxs):\n",
                "        # ref_F = rand_F(Path[0], F)\n",
                "        \n",
                "        # df, qs = sample_J_traj(Path, ref_F[rand], model, robot) \n",
                "        df, qs = sample_J_traj(Path, ref_F[rand], model, robot) \n",
                "        print(\"=\"*6 + str(rand) + f\"=({ref_F[rand]})\" + \"=\"*6)\n",
                "        print(df.describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "path_following(robot=panda, Path_dir=Path_dir, model=iflow, knn=knn, F=F, num_traj=3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "path_following(robot=panda, Path_dir=Path_dir, model=nflow, knn=knn, F=F)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ikflow.utils import set_seed\n",
                "from ikflow.model_loading import get_ik_solver"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "file_names = ['ang_errs_avg', 'ang_errs_min', 'F_avg', 'F_min', 'ikflow_ang', 'ikflow_l2', 'l2_errs_avg', 'l2_errs_min', 'n_evals']\n",
                "exp_5_fig_dir = config.traj_dir + f'figs/exp_5_{datetime.now().strftime(\"%m%d%H%M\")}/'\n",
                "if not os.path.exists(path=exp_5_fig_dir):\n",
                "    os.makedirs(exp_5_fig_dir)\n",
                "set_seed()\n",
                "num_trails = 100\n",
                "num_generation = 100\n",
                "num_ikflow_trails = num_generation\n",
                "num_solutions = 500\n",
                "# Build IKFlowSolver and set weights\n",
                "ik_solver, hyper_parameters = get_ik_solver(\"panda__full__lp191_5.25m\")\n",
                "robot = ik_solver.robot\n",
                "panda = Robot(verbose=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "target_poses = load_numpy(file_path=traj_dir + 'ee_traj.npy')\n",
                "# traj_dir, target_poses = get_target_poses(robot=panda)\n",
                "exp_dir = traj_dir + f'exp_5_single_obj/'\n",
                "data = {fn: [] for fn in file_names}\n",
                "\n",
                "# -> unrefined solutions\n",
                "mean_position_errors = np.zeros((num_ikflow_trails,))\n",
                "mean_ang_errs = np.zeros((num_ikflow_trails,))\n",
                "for ikflow_i in range(num_ikflow_trails):\n",
                "    solutions, position_errors, _, _, _, _ = ik_solver.solve_n_poses(\n",
                "        target_poses, \n",
                "        refine_solutions=False, \n",
                "        return_detailed=True)\n",
                "\n",
                "    iksols = solutions.detach().cpu().numpy()\n",
                "    df = eval_J_traj(robot=panda, J_traj=solutions.to('cpu'), P_path=target_poses, position_errors=position_errors)\n",
                "    mean_position_errors[ikflow_i], mean_ang_errs[ikflow_i]  = df.mean().values\n",
                "data['ikflow_l2'] = mean_position_errors\n",
                "data['ikflow_ang'] = mean_ang_errs    \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.DataFrame(mean_position_errors)\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.DataFrame(mean_ang_errs)\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
